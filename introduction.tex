\setcounter{secnumdepth}{0}
\chapter{Preliminaries}
\section{Introduction}
Bellman's ``Principle of Optimality'' and the characterization of dynamic programming is one of the most important results in computing~\cite{bellman2013dynamic}. Its importance stems from the ubiquity of Markovian decision processes (MDPs), which formalize a wide range of problems from path planning to scheduling~\cite{howard1966dynamic}. In the most abstract form, there is an agent who makes a sequence of decisions to effect change on a system that processes these decisions and updates its internal state. 
The process is ``Markovian'' in the sense that the system's current state completely determines its future progression. As an example of an MDP, one might have to plan a sequence of motor commands to a robot to control it to a target position. Or, one might have to schedule a sequence of cluster computing tasks while avoiding double scheduling on a node. 
The solution to a MDP is a decision making policy---the optimal decision to make given any current state of the system. 




Unfortunately, finding optimal solutions to general MDP problems is often computationally impractical. 





Over the last few years, approximations based on the combination deep neural networks and reinforcement learning, or Deep RL, have emerged as a pragmatic solution~\cite{mnih2015human,silver2017mastering}. Deep RL only assumes black-box access to the system model, and relies on iteratively querying the model to optimize its decision polices. This allows the Deep RL family of algorithms to be extremely general and widely applicable across many different problem settings. More importantly, there are reductions to optimal control, classical graph search, and supervised learning. The impact of general-purpose Deep RL algorithms cannot be understated; they promise a unified approach to many planning, control, and learning problems. 
From a Software Engineering perspective, this unification allows the community to develop optimized libraries for Deep RL rather each domain designing/maintaining problem-specific algorithms.

However, practical progress in Deep RL has been significantly slower. 
While Deep RL research has made significant breakthroughs over the last several years, there has been a notable lack industrial production applications outside of the canonical academic benchmark simulators (e.g., with few exceptions \cite{mirhoseini2017device}). 
There are several arguments for why this is the case including: (1) the algorithms are still not sample-efficient and struggle when querying the system is a bottleneck~\cite{sunderhauf2018limits, stoica2017berkeley}, (2) existing implementations are sensitive to hyper-parameters and generally difficult to reproduce~\cite{islam2017reproducibility, henderson2017deep}, (3) existing implementations are not significantly more efficient than random parameter-space search~\cite{mania2018simple, salimans2017evolution}, and (4) performance in the AlphaGo and Atari domains are not indicative of settings where there are imperfect observations~\cite{stoica2017berkeley, sunderhauf2018limits}.

These criticisms focus on applying Deep RL in its purest form; where no \emph{a priori} information about valuable decision sequences is available to the agent.
In contrast, classical algorithmic methods heavily exploit heuristics, prior knowledge about the system, and the structure of typical problems in the domain.
This poses an existential dilemma for Deep RL---either one sacrifices generality by incorporating domain knowledge into the Deep RL algorithm or one pays the price of essentially random behavior until a viable decision policy is found. Building and maintaining domain-specific Deep RL implementations defeats the unifying premise of Deep RL.

This dissertation argues that general-purpose Deep RL implementations can be made to exploit many forms of domain-specific structure without algorithmic modification. In several domains of interest, a limited amount of expert knowledge is available, e.g., a human teleoperator can guide the motion of a robot or a classical search algorithm can be applied. 
My thesis is that \emph{it is possible to learn important structural features of a sequential prediction task from such expert data to significantly improve the sample-efficiency, stability, and applicability of Deep RL.} 
With this additional supervision, the search process can be restricted to those sequences that are similar to the supervision provided by the expert.
My work over the last 6 years explores this architecture in several Deep RL systems for control of imprecise cable-driven surgical robots, automatically synthesizing data-cleaning programs to meet quality specifications, and generating efficient execution plans for relational queries. I describe algorithmic contributions, theoretical analysis about the implementations themselves, and the architecture of the RL systems.


\vspace{0.75em} \noindent \textbf{Problem Setting: }
A discrete-time discounted Markov Decision Process (MDP) is described by a 6-tuple $\langle \mathcal{S}, \mathcal{A}, p_0, p, R, \gamma \rangle$, where $\mathcal{S}$ denotes the state space, $\mathcal{A}$ the action space, $p_0$ the initial state distribution, $p(s_{t+1} \mid s_{t}, a_{t})$ the state transition distribution, $R(s_t, a_t) \in \mathbb{R}$ is the reward function, and $\gamma \in [0,1)$ the discount factor. 
The objective of an MDP is to find a decision policy, a probability distribution over actions $\pi: \mathcal{S} \mapsto \Delta(A)$.
A policy $\pi$ induces the distribution over trajectories:
\[
P_\pi(\xi) = p_0(x_0) \prod_{t=0}^{T-1} \pi(a_t \mid s_t) p(s_{t+1} \mid s_{t}, a_{t}).
\]
The \emph{return} of a policy is its expected total discounted reward over trajectories
\[
V_\pi = \mathbf{E}_{\xi \sim P_\pi}\left[\sum_{t=0}^{T-1} \gamma^t R(s_t,a_t)\right].
\]
The objective is to find a policy in a class of allowed policies $\pi^* \in \Pi$ to maximize the return:
\begin{equation}
\pi^* = \arg \max_{\pi \in \Pi} V_\pi 
\label{eq:main}
\end{equation}
We assume access to an expert who samples from an unknown policy $\hat{\pi} \approx \pi^*$ the optimal policy; these samples are called demonstrations. A demonstration $d$ is a sequence of tuples of states, actions, and rewards:
\[d = [(s_0,a_0,r_0),(s_1,a_1,r_1),...,(s_T,a_T,r_2)]\]

\vspace{0.25em}

\begin{problem}[Reinforcement Learning with Demonstrations]
Given an MDP and a set of demonstration trajectories $D = \{d_1,...,d_N\}$ from a supervisor, return a policy $\pi^*$ that maximizes the cumulative return of the MDP.
\end{problem}


\vspace{0.75em} \noindent \textbf{Contributions: }  This dissertation consists of algorithmic, theory, and systems contributions and are summarized below:


\begin{enumerate}
    \item I present a new learning framework for learning parametrized control hierarchies from expert data (Chapter 2). This decomposes the action space into a discrete set of skills relevant to the task. New problem instances search over compositions of the discrete skills rather than arbitrary sequences of actions. The motivating application is a system for multi-step robot manipulation.
    
    \item I present a variant of this framework for exploiting ``third person'' demonstrations data in reinforcement learning where only the states that the agent visits are visible (Chapter 3). The motivating application is learning from expert surgeons in surgical robotics.
    
    \item I present a system for synthesizing data cleaning programs to enforce data base integrity constraints. The unique part of this system is that it self-supervises where an exhaustive search algorithm provides demonstrations to make itself incrementally less exhaustive (Chapter 4). 
    
    \item I present a system for synthesizing surgical thin tissue tensioning policies using deep reinforcement learning. To improve the training time of the system, it initializes its search with an analytical approximation (Chapter 5). 
    
    \item Recognizing that Q-Learning is a form of approximate dynamic programming, I present results where we replace the classical SQL query optimizer with a Deep RL algorithm (Chapter 6). The algorithm is initialized with demonstrations from a classical dynamic programming optimizer.
    
\end{enumerate}







