\chapter{Discovery of Parametrized Options}
\setcounter{secnumdepth}{1}
In high-dimensional or combinatorial action spaces, the vast majority of action sequences are irrelevant to the task at hand.
Therefore, purely random exploration is likely to be prohibitively wasteful.
What makes many sequential problems particularly challenging for is that all of these random action sequences are, in a sense, equally irrelevant. 
This means that there might not be any signal in the sampled data that the Q-learning algorithm can exploit.
Until the agent serendipitously discovers such a sequence, no learning can occur.

In the first section of this dissertation, I explore this problem in the context of robotics, self-play in atari games, and program imitation. 
The basic insight is that while the space of all action sequences $\mathcal{A}^T$ is very large, there is often a much smaller subset of those that are potentially relevant to typical problem instances $\mathcal{A}_{relevant} \subset \mathcal{A}^T$. 
Given a small amount of expert information, I describe techniques for learning $\mathcal{A}_{relevant}$ from data.

How do we parametrize $\mathcal{A}_{relevant}$?
One approach is to describe the agent's actions as a composition higher-level behaviors called~\cite{suttonPS99}, each consisting of a control policy for one region of the state space, and a termination condition recognizing leaving that region. This augmentation naturally defines a hierarchical structure of high-level \emph{meta-control} policies that invoke lower-level options to solve sub-tasks.
This leads to a ``divide and conquer'' relationship between the levels, where each option can specialize in short-term planning over local state features, and the meta-control policy can specialize in long-term planning over slowly changing state features.
This means that any search can be restricted to the action sequences that can be formed as a composition of skills.


Abstractions for decomposing an MDP into subtasks have been studied in the area of hierarchical reinforcement learning (HRL)~\cite{parr98,suttonPS99,barto03}.
Early work in hierarchical control demonstrated the advantages of hierarchical structures by handcrafting hierarchical policies~\cite{brooks1986robust} and by learning them given various manual specifications: state abstractions~\cite{dayanH92,hengst02,kolterAN07,konidarisB07}, a set of waypoints~\cite{kaelbling93}, low-level skills~\cite{huberG97,baconP15,liaw17composing}, a set of finite-state meta-controllers~\cite{parrR97}, or a set of subgoals~\cite{suttonPS99,dietterich00}.
The key abstraction in HRL is the ``options framework''~\cite{suttonPS99}, which defines a hierarchy of increasingly complex meta-actions.
 An option represent a lower-level control primitive that can be invoked by the \emph{meta-control} policy at a higher-level of the hierarchy, in order to perform a certain subroutine (a useful sequence of actions).
The meta-actions invoke specialized policies rather than just taking primitive actions.

 Formally, an option $h$ is described by a triplet \[\langle \mathcal{I}_h, \pi_h, \psi_h \rangle,\] where $\mathcal{I}_h \subset \mathcal{X}$ denotes the initiation set, $\pi_h(u_t \mid x_t)$ the control policy, and $\psi_h(s_t) \in [0,1]$ the termination policy. When the process reaches a state $s \in \mathcal{I}_h$, the option $h$ can be invoked to run the policy $\pi_h$. After each action is taken and the next state $s'$ is reached, the option $h$ terminates with probability $\psi_h(s')$ and returns control up the hierarchy to its invoking level. The options framework enables multi-level hierarchies to be formed by allowing options to invoke other options. A higher-level meta-control policy is defined by augmenting its action space $\mathcal{A}$ with the set $\mathcal{H}$ of all lower-level options. 

The options framework has been applied in robotics~\cite{konidarisKGB12, krishnan2016swirl, sermanet2016unsupervised} and in the analysis of biological systems~\cite{botvinick08,botvinick2009hierarchically,solway2014optimal,zacksKEH11,whitenFBL06}.Since then, the focus of research has shifted towards discovery of the hierarchical structure itself, by: trading off value with description length~\cite{thrunS94}, identifying transitional states~\cite{mcgovernB01,menacheMS02,simsekB04,stolle04,lakshminarayananKKR16}, inference from demonstrations~\cite{buiVW02,krishnan2015transition,daniel2012hierarchical,krishnan2016swirl}, iteratively expanding the set of solvable initial states~\cite{konidarisB09,konidarisKGB12}, policy gradient~\cite{LevyS11}, trading off value with informational constraints~\cite{geneweinLGB15,foxMT16,jonssonG16,florensaDA17}, active learning~\cite{hamidiTGF15}, or recently value-function approximation~\cite{baconHP16,heess2016learning,sharmaLR17}.


