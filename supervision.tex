\section{Supervision}
It should now be clear that discovering a viable policy purely through exploration can be extremely difficult, especially if there are narrow criteria for task success.
In long tasks, it can be very difficult to quantify the advantage of taking a particular action without a large number of repeated trials. This section describes the notion of expert information, or demonstrations, that can be leveraged to structure the search problem.

All of the proposed techniques in the following sections 

\subsection*{Imitation Learning}
The study of learning control policies from demonstrations is called imitation learning~\cite{osa2018algorithmic}. In the most basic form, a demonstration is defined as follows consisting of a sequence of state, action, reward tuples. 

\begin{definition}[Demonstration]
A demonstration $\mathbf{d}$ is a sequence of tuples of states, actions, and rewards:
\[\mathbf{d} = [(s_0,a_0,r_0),(s_1,a_1,r_1),...,(s_T,a_T,r_2)]\]
\end{definition}

Imitation learning studies the problem of synthesizing a control policy from such demonstrations.

\begin{problem}[Policy Search]
Given an MDP and a set of demonstration trajectories $D = \{d_1,...,d_N\}$ from a supervisor, return a policy $\pi: S \mapsto \Delta(A)$ that maximizes the cumulative reward of the MDP.
\end{problem}

Noting this definition, this is exactly the data needed for the approximate q-learning approaches described in the previous section.
The DQN reinforcement learning algorithm can be thought of as bootstrapping demonstrations from its current best guess of the Q-Function~\cite{mnih2015human}. Thus, the difference between reinforcement learning and imitation learning is purely semantics when state, actions, and rewards are observed from the data generating agent. However, the distinction becomes more apparent when the nature of demonstration change. Depending on what information is available, different algorithms might be used. 

\subsubsection{Supervised Imitation Learning}
A popular problem setting in robotics is where the reward function is not available to the demonstrator. Consider a worker in a factory moving a robot with a joystick. Here the objective of the worker is unknown but simply a trajectory of states and action. Similarly, in programming-by-examples, one only observes input and output data and not a complete specification of the program.

\begin{definition}[Reward-Free Demonstration]
A reward-free demonstration is a sequence of tuples of states and actions:
\[\mathbf{d} = [(s_0,a_0,\_),(s_1,a_1,\_),...,(s_T,a_T,\_)]\]
\end{definition}



\subsubsection{Inverse Reinforcement Learning}


\subsubsection{Unsupervised Imitation Learning}
While the state-action is typical of the imitation learning literature~\cite{osa2018algorithmic}, one may have a more limited access to a limited supervisor where only the states are observed. The actions that the supervisor applied are latent.

\begin{definition}[State Demonstration]
A state demonstration $d$ is a sequence of tuples of states and actions:
\[d = [(s_0,\_,\_),(s_1,\_,\_),...,(s_T,\_,\_)]\]
\end{definition}

The state demonstration setting differs from what is classically studied in imitation learning. 
This can happen when the demonstration modality differs from the execution setting, e.g., learning from third person videos, motion capture of a human doing a task, or kinesthetic demonstrations. 
In this problem setting, the data cannot be directly used to derive a controller since the actions are not visible.
The either actions must be implicitly inferred from a inverse dynamics model or the data must be used to learn general task structure.
Examples include the work on Time Constrastive Networks~\cite{sermanet2017time}, third-person imitation learning~\cite{stadie2017third}, and watch-and-patch~\cite{wu2018watch}.

\subsection*{Between Imitation and Reinforcement}




