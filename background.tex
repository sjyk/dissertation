\vspace{0.5em}

\section{Background and Related Work}
A discrete-time discounted Markov Decision Process (MDP) is described by a 6-tuple $\langle \mathcal{S}, \mathcal{A}, p_0, p, R, \gamma \rangle$, where $\mathcal{S}$ denotes the state space, $\mathcal{A}$ the action space, $p_0$ the initial state distribution, $p(s_{t+1} \mid s_{t}, a_{t})$ the state transition distribution, $R(s_t, a_t) \in \mathbb{R}$ is the reward function, and $\gamma \in [0,1)$ the discount factor. 
The objective of an MDP is to find a decision policy, a probability distribution over actions $\pi: \mathcal{S} \mapsto \Delta(A)$.
A policy $\pi$ induces the distribution over trajectories $\xi = [(s_0,a_0),(s_1,a_1),...,(s_N,a_N)]$:
\[
P_\pi(\xi) = p_0(x_0) \prod_{t=0}^{T-1} \pi(a_t \mid s_t) p(s_{t+1} \mid s_{t}, a_{t}).
\]
The \emph{value} of a policy is its expected total discounted reward over trajectories
\[
V_\pi = \mathbf{E}_{\xi \sim P_\pi}\left[\sum_{t=0}^{T-1} \gamma^t R(s_t,a_t)\right].
\]
The objective is to find a policy in a class of allowed policies $\pi^* \in \Pi$ to maximize the return:
\begin{equation}
\pi^* = \arg \max_{\pi \in \Pi} V_\pi 
\label{eq:main}
\end{equation}

\subsection{The Ubiquity of Markov Decision Processes}
While it is true that many systems of interest are not Markov and do not offer direct observation of their internal states, the MDP actually covers a substantial number of classical problems in Computer Science. Consider the following reductions:

\vspace{0.5em}\noindent \textbf{Shortest Path Graph Search: } A graph search problem instance is defined as follows. Let $G=(V,E)$ be a graph with vertices $V$ and edges $E$, and let $q \in V$ denote a starting vertex and $t \in V$ denote a target vertex. Find a path connected by edges from $q$ to $t$. In MDP notation, we can consider a hypothetical agent whose state is a pointer to a vertex, its actions are moving to an adjacent vertex, the transition executes this move, and its reward function is an indicator of whether the state is $t$. More formally, $\mathcal{S} = V$, $\mathcal{A} = S \times S$, $R = \mathbf{1}(s_t = t)$, and $p_0 = q$. A discount factor of $\gamma = 1$ specifies that any path is optimal, and $\gamma < 1$ specifies that shorter paths are preferred.

\vspace{0.5em}\noindent \textbf{Supervised Learning: } In empirical risk minimization for supervised learning, one is given a set of tuples $(X,Y) = {(x_0,y_0),...,(x_N,y_N)}$ and the objective is to find a function that minimizes $f: X \mapsto Y$ that minimizes some point-wise measure of disagreement called a loss function $\sum_{i=0}^N \ell(f(x_i),y_i)$. In MDP notation, this is a ``stateless'' problem.  The agent's state is a randomly chose example $\mathcal{S} = X$, its action space is $Y$, and the reward function is the loss function.

\vspace{0.5em}\noindent \textbf{Optimal Control: } Optimal control problems also constitute Markov Decision Process problems:
\[
\min_{a_1,...,a_T} \sum_{i=1}^T \gamma^i \cdot J(s_i, a_i) 
\]
\[
\text{subject to:} ~~ s_{i+1} = f(s_i, a_i)
\]
\[
a_i \in \mathcal{A} ~~ s_i \in \mathcal{S} ~~ s_1 = \mathbf{c}
\]
The problem is to select $T$ decisions where each decision $a_i$ resides in an action space $\mathcal{A}$. The decision making problem is stateful where the world has an initial state $s_1$ and this state is affected by every decision the decision-making agent selects through the transition model $f(s_i, a_i)$ which transitions the state to another state in the set $\mathcal{S}$. The objective is to optimize the cumulative cost of these decisions $J(s_i,a_i)$ potentially subject to an exponential discount $\gamma$ that controls a bias towards short term or long term costs.

\subsection{Reinforcement Learning (RL)}
The reinforcement learning setting further assumes ``black box'' (also called oracular) access to the state transition distribution $p$ and the reward function $R$.
This means the optimization algorithm is only allowed queries of the following form:
\[
q_t: s_t, a_t \rightarrow \textsf{system()} \rightarrow s_{t+1}, r_{t}
\]
This dissertation terms any algorithm that satisfies this query model as RL.
The number of such queries issued by an RL algorithm is called its sample-complexity. 
This relaxes the restriction of any analytic knowledge about the structure of $R$ or $p$, and only requires a system model that can be queried (e.g., written in code, implemented as physical system).
The focus of this dissertation is on cases where \emph{ab initio} RL has a prohibitive sample complexity due to high-dimensional action spaces or long time horizons.
 With no prior knowledge of which actions lead to rewards any RL algorithm has to essentially start with random decisions, and even when the algorithm observes a positive signal it has no notion of directionality. 

\subsection{Imitation Learning}
A contrasting approach to RL is imitation learning~\cite{osa2018algorithmic}, where one assume access to an supervisor who samples from an unknown policy $\hat{\pi} \approx \pi^*$ the optimal policy; these samples are called demonstrations. Rather than querying the system to optimize the policy, the problem is to imitate the supervisor as best as possible.
 Consider a worker in a factory moving a robot with a joystick. Here the objective of the worker is unknown but simply a trajectory of states and action. Similarly, in programming-by-examples, one only observes input and output data and not a complete specification of the program. 
In the most basic form, such a problem reduces to Maximum Likelihood Estimation. 

A policy $\pi_\theta(a_t \given s_t)$ defines the distribution over controls given the state, parametrized by $\theta\in\Theta$. In Behavior Cloning (BC), one trains the parameter $\theta$ so that the policy fits the dataset of observed demonstrations and imitates the supervisor. For example, we can maximize the log-likelihood $L[\theta;\xi]$ that the stochastic process induced by the policy $\pi_\theta$ assigns to each demonstration trajectory $\xi$:
\eq{
L[\theta;\xi] = \log p_0(s_0) + \sum_{t=0}^{T-1} \log(\pi_\theta(a_t \given s_t)p(s_{t+1} \given s_t, a_t)).
}
When $\log\pi_\theta$ is parametrized by a deep neural network, we can perform stochastic gradient descent by sampling a batch of transitions, e.g. one complete trajectory, and computing the gradient
\eq{
\nabla_\theta L[\theta;\xi] = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \given s_t).
}
Note that this method can be applied model-free, without any knowledge of the system dynamics $p$.

Another approach to the imitation setting is Inverse Reinforcement Learning~\cite{abbeel2011inverse,ziebart2008maximum,ng2000algorithms}. This approach infers a reward function from the observed data (and possibly the system dynamics)--thus, reducing the problem to the original RL problem setting. \cite{abbeel2004apprenticeship} argue that the reward function is often a more concise representation of task than a policy. As such, a concise reward function is more likely to be robust to small perturbations in the task description. 
The downside is that the reward function is not useful on its own, and ultimately a policy must be retrieved. In the most general case, an RL algorithm must be used to optimize for that reward function~\cite{abbeel2004apprenticeship}.

\subsection{Reinforcement Learning with Demonstrations}
However, imitation learning places a significant burden on a supervisor to exhaustively cover the scenarios the robot may encounter during execution~\cite{laskey2017iterative}.
To address the limitations on either extreme of imitation and reinforcement, this dissertation proposes a hybrid of the exploration and demonstration learning paradigms. 

\begin{problem}[Reinforcement Learning with Demonstrations]
Given an MDP and a set of demonstration trajectories $D = \{\xi_1,...,\xi_N\}$ from a supervisor, return a policy $\pi^*$ that maximizes the cumulative reward of the MDP with a reinforcement learning algorithm.
\end{problem}

This is a problem setting that has been studied by a few recent works.
In Deeply Aggravated~\cite{sun2017deeply},  the expert must provide a value function in addition to actions, which ultimately creates an algorithm similar to Reinforcement Learning.
This basic setting is also similar to the problem setting consider in~\cite{piot2014boosted}.
\cite{subramanian2016exploration} consider a model where the random search policy of the algorithm is guided by expert demonstrations.
Rather than manipulating the search strategy, \cite{brys2015reinforcement} modify the ``shape'' the reward function to match trajectories seen in demonstration.
This is an idea that has gotten recent traction in the robot learning community~\cite{duan2017one, james2017transferring, hester2017deep}. 

This recent work raises a central question: what should be learned from demonstrations to effectively bootstrap reinforcement learning?
One approach is using imitation learning to learn a policy which initializes the search in RL.
While this can be a very effective strategy, it neglects other structure that is latent in the demonstrations like are there common sequences of actions that tend to occur or do parts of the task decompose into simpler units.
In the high-dimensional setting, it is crucial to exploit such structure and this dissertation shows that different aspects of the structure such as task decomposition, action hierarchy, and action relevancy can be cast as Bayesian latent variable problems. I explore learning this structure in detail and the impact of learning this structure on several RL problems. 




