\section{Learning Options From Demonstrations}
Since designing such hierarchical structures manually is infeasible in high-dimensional spaces, we need algorithms to discover them. Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies~\cite{kulkarni2016hierarchical, heess2016learning, baconHP16}, while others are inefficient for learning expressive representations~\cite{daniel2012hierarchical,lakshminarayananKKR16,hamidiTGF15,buiVW02}.

We introduce the \emph{Discovery of Deep Options (DDO)}, an algorithm for efficiently discovering deep hierarchies of deep options. DDO is a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories (sequences of states and actions) provided either by a supervisor or by roll-outs of previously learned policies. These demonstrations need not be given by an optimal agent, but it is assumed that they are informative of the preferred actions to take in each visited state, and are not just random walks. DDO is an inference algorithm that applies to the \textbf{supervised demonstration setting}.

Given a set of trajectories, the algorithm discovers a fixed, predetermined number of options that are most likely to generate the observed trajectories. Since an option is represented by both a control policy and a termination condition, our algorithm simultaneously (1) infers option boundaries in demonstrations which segment trajectories into different control regimes, (2) infers the meta-control policy for selecting options as a mapping of segments to the option that likely generated them, and (3) learns a control policy for each option, which can be interpreted as a soft clustering where the centroids correspond to prototypical behaviors of the agent.

\subsection{Deep Discovery of Deep Options}
We generalize standard IL to hierarchical control, by introducing Hierarchical Behavioral Cloning (HBC) model. In HBC, the meta-control signals that form the hierarchy are unobservable, latent variables of the generative model, that must be inferred.

Consider a trajectory $\xi=(s_0,a_0,s_1,\ldots,s_T)$ that is generated by a two-level hierarchy. The low level implements a set $\C H$ of options $\lan \pi_h, \psi_h \ran_{h\in\C H}$. The high level implements a meta-control policy $\eta(h_t \given s_t)$ that repeatedly chooses an option $h_t\sim\eta(\cdot|s_t)$ given the current state, and runs it until termination. Our hierarchical generative model is:
\begin{algorithmic}
    \State Initialize $t\gets0$, $s_0 \sim p_0$, $b_0 \gets 1$
    \For{$t\gets0,\ldots,T-1$}
        \If{$b_t=1$}
            \State Draw $h_t \sim \eta(\cdot \given s_t)$
        \Else\Comment{$b_t=0$}
            \State Set $h_t \gets h_{t-1}$
        \EndIf
        \State Draw $a_t \sim \pi_{h_t}(\cdot \given s_t)$
	\State Draw $s_{t+1} \sim p(\cdot \given s_t, a_t)$
	\State Draw $b_{t+1} \sim \R{Ber}(\psi_{h_t}(s_{t+1}))$
    \EndFor
\end{algorithmic}


\subsubsection{Expectation-Gradient Algorithm}
\slab{eg}
We denote by $\theta$ the vector of parameters for $\pi_h$, $\psi_h$ and $\eta$. For example, $\theta$ can be the weights and biases of a feed-forward network that computes these probabilities. This generic notation allows us the flexibility of a completely separate network for the meta-control policy and for each option, $\theta=(\theta_\eta,(\theta_h)_{h\in\C H})$, or the efficiency of sharing some of the parameters between options, similarly to a Universal Value Function Approximator~\cite{schaul2015universal}.

We want to find the $\theta\in\Theta$ that maximizes the log-likelihood assigned to a given dataset of trajectories. The likelihood of a trajectory depends on the latent sequence $\zeta = (b_0,h_0,b_1,h_1,\ldots,h_{T-1})$ of meta-actions and termination indicators, and in order to use a gradient-based optimization method we rewrite the gradient using the following \emph{EG-trick}:
\eq{
\nabla_\theta L[\theta;\xi] &= \nabla_\theta \log \p_\theta(\xi) = \frac 1 {\p_\theta(\xi)} \sum_{\zeta\in(\{0,1\}\times\C H)^T} \nabla_\theta \p_\theta(\zeta,\xi) \\
&= \sum_\zeta \frac {\p_\theta(\zeta,\xi)} {\p_\theta(\xi)} \nabla_\theta \log \p_\theta(\zeta,\xi) = \E_{\zeta|\xi;\theta}[\nabla_\theta \log \p_\theta(\zeta,\xi)],
}
which is the so-called \emph{Expectation-Gradient} method \cite{salakhutdinov2003optimization,mclachlan2007algorithm}. $\theta^-$ denotes the current parameter taken as fixed outside the gradient.

The generative model in the previous section implies the likelihood
\eq{
\p_\theta(\zeta,\xi) = p_0(s_0) \delta_{b_0=1}\eta(h_0 \given s_0) \prod_{t=1}^{T-1} \p_\theta(b_t, h_t \given h_{t-1}, s_t) \prod_{t=0}^{T-1} \pi_{h_t}(a_t \given s_t) p(s_{t+1} \given s_t, a_t) ,
}
with
\eq{
\p_\theta(b_t {=} 1, h_t \given h_{t-1}, s_t) &= \psi_{h_{t-1}}(s_t) \eta(h_t \given s_t) \\
\p_\theta(b_t {=} 0, h_t \given h_{t-1}, s_t) &= (1 - \psi_{h_{t-1}}(s_t)) \delta_{h_t = h_{t-1}}.
}
$\delta_{h_t = h_{t+1}}$ denotes the indicator that $h_t = h_{t+1}$.

Applying the EG-trick and ignoring the terms that do not depend on $\theta$, we can simplify the gradient to:
\eq{
\nabla_\theta L[\theta;\xi] = \E_{\zeta|\xi;\theta}\Bigg[\nabla_\theta \log \eta(h_0 \given s_0) + \sum_{t=1}^{T-1}\nabla_\theta \log \p_\theta(b_t, h_t \given h_{t-1}, s_t) + \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{h_t}(a_t \given s_t) \Bigg].
}
%\eq{
%= {}& \mrlap{\sum_{h\in\C H} \p_\theta(h_0=h \given \xi) \nabla_\theta \log \eta_\theta(h \given s_0)} \\
%& + \mrlap{\sum_{t=0}^{T-1}\sum_{h\in\C H} \p_\theta(h_t=h \given \xi) \nabla_\theta \log \pi_{\theta;h}(a_t \given s_t) }\\
%& + \mrlap{\sum_{t=0}^{T-1}\sum_{h\in\C H} \p_\theta(h_t=h, b_t=1 \given \xi) \nabla_\theta \log \psi_{\theta;h}(s_{t+1})} \\
%& + \mrlap{\sum_{t=0}^{T-1}\sum_{h\in\C H} \p_\theta(h_t=h, b_t=0 \given \xi) \nabla_\theta \log (1 - \psi_{\theta;h}(s_{t+1}))} \\
%& + \mrlap{\sum_{t=0}^{T-1}\sum_{h\in\C H} \p_\theta(b_t=1, h_{t+1}=h \given \xi) \nabla_\theta \log \eta_\theta(h \given s_{t+1}).}
%}
The log-likelihood gradient can therefore be computed as the sum of the log-probability gradients of the various parameterized networks, weighed by the marginal posteriors
\eq{
u_t(h) &= \p_\theta(h_t {=} h \given \xi) \\
v_t(h) &= \p_\theta(b_t {=} 1, h_t {=} h \given \xi) \\
w_t(h) &= \p_\theta(h_t {=} h, b_{t+1} {=} 0 \given \xi).
}
In the Expectation-Gradient algorithm, the E-step computes $u$, $v$ and $w$, and the G-step updates the parameter with a gradient step, namely
\eq{
\nabla_\theta L[\theta;\xi] = \sum_{h\in\C H} \Biggl(& \sum_{t=0}^{T-1}\mrlap{ \Biggl(v_t(h) \nabla_\theta \log \eta(h \given s_t) +  u_t(h)\nabla_\theta \log \pi_h(a_t \given s_t)\Biggr)} \\ 
& + \sum_{t=0}^{T-2} \Biggl((u_t(h)-w_t(h)) \nabla_\theta \log \psi_h(s_{t+1}) + w_t(h) \nabla_\theta \log (1 - \psi_h(s_{t+1})) \Biggr)\Biggr).
}

These equations lead to the natural iterative algorithm. In each iteration, the marginal posteriors $u$, $v$ and $w$ can be computed with a forward-backward message-passing algorithm similar to Baum-Welch~\cite{baum1972equality}, with time complexity $O(|\C H|^2 T)$. Importantly, this algorithm can be performed without any knowledge of the state dynamics. The details of this computation are given in the supplementary material.
Then, the computed posteriors can be used in a gradient descent algorithm to update the parameters:
\eq{
\theta \gets \theta + \alpha \sum_i \nabla_\theta L[\theta;\xi_i].
}
This update can be made stochastic using a single trajectory, uniformly chosen from the demonstration dataset, to perform each update.

Intuitively, the algorithm attempts to jointly optimize three objectives:
\begin{itemize}
    \item Infer the option boundaries in which $b=1$ appears likely relative to $b=0$, as given by $(u-w)$ and $w$ respectively --- this segments the trajectory into regimes where we expect $h$ to persist and employ the same control law; in the G-step we reduce the cross-entropy loss between the unnormalized distribution $(w,u-w)$ and the termination indicator $\psi_h$;
    \item Infer the option selection after a switch, given by $v$; in the G-step we reduce the cross-entropy loss between that distribution, weighted by the probability of a switch, and the meta-control policy $\eta$; and
    \item Reduce the cross-entropy loss between the empirical action distribution, weighted by the probability for $h$, and the control policy $\pi_h$.
\end{itemize}
This can be interpreted as a form of soft clustering. 
The data points are one-hot representations of each $a_t$ in the space of distributions over actions.
Each time-step $t$ is assigned to option $h$ with probability $u_t(h)$, forming a soft clustering of data points.
The G-step directly minimizes the KL-divergence of the control policy $\pi_h$ from the weighted centroid of the corresponding cluster.

Let $\delta_{a_t}(a|s_t)=\delta_{a=a_t}$ be the degenerate ``empirical'' action distribution of step $t$.
The KL divergence of $\pi_h$ from the weighted centroid of the cluster corresponding to option $h$.

\subsubsection{Forward-Backward Algorithm}
\slab{forwardbackward}
Despite the exponential domain size of the latent variable $\zeta$, Expectation-Gradient for trajectories allows us to decompose the posterior $\p_\theta(\zeta \given \xi)$ and only concern ourselves with each marginal posterior separately. These marginal posteriors can be computed by a forward-backward dynamic programming algorithm, similar to Baum-Welch~\cite{?}.

Omitting the current parameter $\theta$ and trajectory $\xi$ from out notation, we start by computing the likelihood of a trajectory prefix
\eq{
\phi_t(h) = \p(s_0,a_0,\ldots,s_t,h_t=h),
}
using the forward recursion
\eq{
\phi_0(h) &= p_0(s_0) \eta(h \given s_0) \\
\phi_{t+1}(h') &= \sum_{h\in\C H} \nu_t(h) \pi_h(a_t \given s_t) p(s_{t+1} \given s_t, a_t) \p(h' \given h, s_{t+1}),
}
with
\eq{
\p(h' \given h, s_{t+1}) = \psi_h(s_{t+1}) \eta(h' \given s_{t+1}) + (1 - \psi_h(s_{t+1})) \delta_{h, h'}.
}
We similarly compute the likelihood of a trajectory suffix
\eq{
\omega_t(h) = \p(a_t,s_{t+1},\ldots,s_T \given s_t, h_t=h),
}
using the backward recursion
\eq{
\omega_T(h) &= 1 \\
\omega_t(h) &= \pi_h(a_t \given s_t) p(s_{t+1} \given s_t, a_t) \sum_{h'\in\C H} \p(h' \given h, s_{t+1}) \omega_{t+1}(h').
}

We can now compute our target likelihood using any $0\le t\le T$
\eq{
\p(\xi) = \sum_{h\in\C H} \p(\xi, h_t=h) = \sum_{h\in\C H} \phi_t(h) \omega_t(h).
}
The marginal posteriors are
\eq{
u_t(h) &= \frac{\phi_t(h) \omega_t(h)} {\p(\xi)} \\
v_t(h) &= \frac{\phi_t(h) \pi_h(a_t \given s_t) p(s_{t+1} \given s_t, a_t) \psi_h(s_{t+1}) \sum_{h'\in\C H} \eta(h' \given s_{t+1}) \omega_{t+1}(h')} {\p(\xi)} \\
w_t(h') &= \frac{\sum_{h\in\C H} \phi_t(h) \pi_h(a_t \given s_t) p(s_{t+1} \given s_t, a_t) \psi_h(s_{t+1}) \eta(h' \given s_{t+1}) \omega_{t+1}(h')} {\p(\xi)}.
}
Note that the constant $p_0(s_0) \prod_{t=0}^{T-1} p(s_{t+1} \given s_t, a_t)$ is cancelled out in these normalizations. This allows us to omit these terms during the forward-backward algorithm, which can thus be applied without any knowledge of the dynamics.


\subsubsection{Stochastic Variant}
We may collect a large number of trajectories making it difficult to scale the EG algorithm.
The expensive step in this algorithm is usually the forward-backward calculation, which is an $O(h^2T)$ operation.
To address this problem, we can apply a stochastic variant of the EG algorithm, which optimizes a single trajectory for each iterate:
\begin{itemize}
    \item \textbf{E-Step: } Draw a trajectory $i$ at random, calculate $q_i(t,h)$, and $b_i(t,h)$ with the forward-backward algorithm.
   \item \textbf{G-Step: } Update the parameters of the policies and the termination conditions:
    \[
\theta_h^{(j+1)} \leftarrow \theta_i^{(j)} - \alpha  \sum_{t=1}^{T} w_i(h,t) \nabla_\theta \log \pi_\theta(a_t \given s_t).
\]
\[
\mu_h^{(j+1)} \leftarrow \mu_i^{(j)} - \alpha \sum_{t=1}^{T} q_i(h,t) \nabla_\mu \log \rho_\mu(\perp \given s_t).
\]
\end{itemize}


\subsubsection{Deeper Hierarchies}
Our ultimate goal is to use the algorithm presented here to discover a multi-level hierarchical structure --- the key insight being that the problem is recursive in nature.
A $D$-level hierarchy can be viewed as a 2-level hierarchy, in which the ``high level'' has a $(D-1)$-level hierarchical structure. 
The challenge is the coupling between the levels; namely, the value of a set of options is determined by its usefulness for meta-control~\cite{foxMT16}, while the value of a meta-control policy depends on which options are available. This potentially leads to an exponential growth in the size of the latent variables required for inference.
The available data may be insufficient to learn a policy so expressive.

We can avoid this problem by using a simplified parametrization for the intermediate meta-control policy $\eta_d$ used when discovering level-$d$ options. In the extreme, we can fix a uniform meta-control policy that chooses each option with probability $\nicefrac 1 {|\C H_d|}$. Discovery of the entire hierarchy can now proceed recursively from the lowest level upward: level-$d$ options can invoke already-discovered lower-level options; and are discovered in the context of a simplified level-$d$ meta-control policy, decoupled from higher-level complexity.
One of the contributions of this work is to demonstrate that, perhaps counter-intuitively, this assumption does not sacrifice too much during option discovery.
An informative meta-control policy would serve as a prior on the assignment of demonstration segments to the options that generated them, but with sufficient data this assignment can also be inferred from the low-level model, purely based on the likelihood of each segment to be generated by each option.

We use the following algorithm to iteratively discover a hierarchy of $D$ levels, each level $d$ consisting of $k_d$ options:
\begin{algorithmic}
    \For{$d=1,\ldots,D-1$}
        \State Initialize a set of options $\C H_d=\{h_{d,1},\ldots,h_{d,k_d}\}$
        \State \alg: train options $\lan \pi_h, \psi_h \ran_{h\in\C H_d}$ with $\eta_d$ fixed
        \State Augment action space $\C A \gets \C A \cup \C H_d$
    \EndFor
    \State Use RL algorithm to train high-level policy
\end{algorithmic}

%To illustrate this point, some of our experiments model the meta-control policy as uniformly choosing each option with probability $\nicefrac 1 {|\C H|}$, only at the option discovery stage.

First, we approximate the high-level policy $\psi$, which selects policies based on the current state, with a i.i.d selection of policies based on only the previous primitive:
\[
\psi \sim \mathbf{P}(h' \given h)
\]
This approximation is so that we do not have to simultaneously learn parameters for the high-level policy, while trying to optimize for the parameters of the policies and termination conditions.
Next, if we collect trajectories from multiple high-level policies, there may not exist a single high-level policy that can capture all of the trajectories.
The approximation allows us to make the fewest assumptions about the structure of this policy.

Using an approximation that $\psi^*$ is a state-independent uniform distribution and sampled i.i.d, this paper will show the we can apply an algorithm similar to typical imitation learning approaches that recovers the most likely parameters to estimate $\pi^*_{1},...,\pi^*_{k}$ and $\{\rho^*_{1},...,\rho^*_{k}\}$. The basic idea is to define a probability that the current $(s_t,a_t)$ tuple is generated by the particular primitive $h$:
\[
w(h,t) = \mathbf{P}[h \given (s_t, a_t), \tau_{0<t}],
\]
and given that the current selected primitive is $h$ probability that the current time-step is termination:
\[
q(h,t) = \mathbf{P}[\perp \given (s_t, a_t), \tau_{0<t}, h].
\]
Given these probabilities, we can define an Expectation-Gradient descent over the parameter vector $\theta$
\eq{
\theta \gets \theta + \alpha \nabla_\theta L[\theta;\xi].
}


\subsubsection{Cross-Validation For Parameter Tuning}
The number of options $k$ is a crucial hyper-parameter of the algorithm.
In simulation experiments, one can roll out the learned hierarchy online and tune the hierarchy based on task success.
Such tuning is infeasible on a real robot as it would require many executions of the learned policy.
We explored whether it is possible to tune the number of options offline.
\alg is based on a maximum-likelihood formulation, which describes the likelihood that the observed demonstrations are generated by a hierarchy parametrized by $\theta$.
However, the model expressiveness is strictly increasing in $k$, causing the optimal training likelihood to increase even beyond the point where the model overfits to the demonstrations and fails to generalize to unseen states.

This likelihood is a proxy for task success.
Therefore, we tune $k$ in a way that maximizes the likelihood.
However, we sometimes encounter a problem similar to over-fitting.
Increasing $k$ actually changes expressiveness the hierarchy, as with more options it can fit to more complicated behaviors.
This means that the tuned parameters may not generalize.

We therefore adopt a cross-validation technique that holds out 10\% of the demonstration trajectories for each of $10$ folds, trains on the remaining data, and validates the trained model on the held out data.
We select the value of $k$ that achieves the highest average log-likelihood over the $10$ folds, suggesting that training such a hierarchical model generalizes well.
We train the final policy over the entire data.
This means the tuned parameter must work well on a hold out set.
We find empirically that the cross-validated log-likelihood serves as a good proxy to actual task performance.

\subsubsection{Vector Quantization For Initialization}
One challenge with DDO is initialization.
When real perceptual data is used, if all of the low-level policies initialize randomly the forward-backward estimates needed for the Expectation-Gradient will be poorly conditioned where there is an extremely low likelihood assigned to any particular observation.
The EG algorithm relies on a segment-cluster-imitate loop, where initial policy guesses are used to segment the data based on which policy best explains the given time-step, then the segments are clustered, and the policies are updated.
In a continuous control space, a randomly initialized policy may not explain any of the observed data well.
This means the small differences in initialization can lead to large changes in the learned hierarchy.

We found that a necessary pre-processing step was a variant of vector quantization, originally proposed for problems in speech recognition. 
We first cluster the state observations using a \textsf{k-means} clustering and train $k$ behavioral cloning policies for each of the clusters.
We use these $k$ policies as the initialization for the EG iterations.
Unlike the random initialization, this means that the initial low level policies will demonstrate some preference for actions in different parts of the state-space.
We set $k$ to be the same as the $k$ set for the number of options, and use the same optimization parameters.